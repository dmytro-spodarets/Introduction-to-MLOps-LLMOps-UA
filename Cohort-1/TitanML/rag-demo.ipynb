{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a simple RAG application with Takeoff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rag](./rag.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import docker\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "HF_TOKEN = os.environ.get(\"TAKEOFF_ACCESS_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titan Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "\"Our research team has issued a comprehensive analysis of the current market trends. Please find the attached report for your review.\",\n",
    "\"The board meeting is scheduled for next Monday at 2:00 PM. Please confirm your availability and agenda items by end of day.\",\n",
    "\"Our quarterly earnings report will be released to the public on the 10th. Senior management is encouraged to prepare for potential investor inquiries.\",\n",
    "\"The due diligence process for the potential merger with XYZ Corp is underway. Please provide any relevant data to the M&A team by Friday.\",\n",
    "\"Please be informed that our compliance department has updated the trading policies. Ensure all employees are aware and compliant with the new regulations.\",\n",
    "\"We're hosting a client seminar on investment strategies next week. Marketing will share the event details for promotion.\",\n",
    "\"The credit risk assessment for ABC Corporation has been completed. Please review the report and advise on the lending decision.\",\n",
    "\"Our quarterly earnings for the last quarter amounted to $3.5 million, exceeding expectations with a 12% increase in net profit compared to the same period last year.\",\n",
    "\"The investment committee meeting will convene on Thursday to evaluate new opportunities in the emerging markets. Your insights are valuable.\",\n",
    "\"Our asset management division is launching a new fund. Marketing will roll out the promotional campaign in coordination with the release.\",\n",
    "\"An internal audit of our trading operations will commence next week. Please cooperate with the audit team and provide requested documents promptly.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker-sdk code\n",
    "\n",
    "def is_takeoff_loading(server_url: str) -> bool:\n",
    "    try:\n",
    "        response = requests.get(server_url + \"/healthz\")\n",
    "        return not response.ok\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        return True\n",
    "\n",
    "def start_takeoff(name, model, backend, device, token=HF_TOKEN):\n",
    "    print(f\"\\nStarting server for {model} with {backend} on {device}...\")\n",
    "    \n",
    "    # Mount the cache directory to the container\n",
    "    volumes = [f\"{Path.home()}/.iris_cache:/code/models\"]\n",
    "    # Give the container access to the GPU\n",
    "    device_requests = [docker.types.DeviceRequest(count=-1, capabilities=[[\"gpu\"]])] if device == \"cuda\" else None\n",
    "    \n",
    "    client = docker.from_env()\n",
    "\n",
    "    image = f\"tytn/takeoff-pro:0.5.0-{'gpu' if device == 'cuda' else 'cpu'}\"\n",
    "    \n",
    "    server_port = 4000\n",
    "    management_port = 4000 + 1\n",
    "    \n",
    "    container = client.containers.run(\n",
    "        image,\n",
    "        detach=True,\n",
    "        environment={\n",
    "            \"TAKEOFF_MAX_BATCH_SIZE\": 10,\n",
    "            \"TAKEOFF_BATCH_DURATION_MILLIS\": 300,\n",
    "            \"TAKEOFF_BACKEND\": backend,\n",
    "            \"TAKEOFF_DEVICE\": device,\n",
    "            \"TAKEOFF_MODEL_NAME\": model,\n",
    "            \"TAKEOFF_ACCESS_TOKEN\": token,\n",
    "            \"TAKEOFF_REDIS_HOST\": \"localhost\",\n",
    "        },\n",
    "        name=name,\n",
    "        device_requests=device_requests,\n",
    "        volumes=volumes,\n",
    "        ports={\"3000/tcp\": server_port, \"3001/tcp\": management_port},\n",
    "        shm_size=\"4G\",\n",
    "    )\n",
    "    \n",
    "    server_url = f\"http://localhost:{server_port}\"\n",
    "    management_url = f\"http://localhost:{management_port}\"\n",
    "    \n",
    "    for _ in range(10):  # Give te server time to init and downlaod models\n",
    "        if not is_takeoff_loading(server_url):\n",
    "            break\n",
    "        print(\"building...\")\n",
    "        time.sleep(3)\n",
    "    print('server ready!')\n",
    "    return server_url, management_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our chatbot model\n",
    "chat_model = 'meta-llama/Llama-2-7b-chat-hf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting server for meta-llama/Llama-2-7b-chat-hf with compress-fast on cuda...\n",
      "building...\n",
      "building...\n",
      "building...\n",
      "server ready!\n"
     ]
    }
   ],
   "source": [
    "# Starting our chatbot\n",
    "takeoff_url, takeoff_mgmt = start_takeoff(\n",
    "            'rag-engine',       #container name \n",
    "            chat_model,         #model name\n",
    "            'compress-fast',    #backend\n",
    "            'cuda'              #device\n",
    "            )\n",
    "\n",
    "# in terminal run: 'docker logs rag-engine' to see status\n",
    "# first time running this may take a while as the image needs to be downlaoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'primary': [{'reader_id': '68fc0c97', 'backend': 'awq', 'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'model_type': 'CAUSAL', 'pids': [40]}]}\n"
     ]
    }
   ],
   "source": [
    "# Check our server details - it maye still be initializing, check the logs\n",
    "response  = requests.get(takeoff_mgmt + '/reader_groups')\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functiuons for printing Server Side Events (SSE) \n",
    "# https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events\n",
    "\n",
    "def print_sse(chunk, previous_line_blank=False):\n",
    "    chunk = chunk.decode('utf-8')\n",
    "    text = chunk.split('data:')\n",
    "    if len(text) == 1:\n",
    "        return True\n",
    "    \n",
    "    text = text[1] \n",
    "    \n",
    "    if not previous_line_blank:\n",
    "        print('\\n')\n",
    "        \n",
    "    print(text, end='')\n",
    "    return False\n",
    "    \n",
    "def stream_response(response):\n",
    "    prev = True\n",
    "    for line in response.iter_lines():\n",
    "        prev = print_sse(line, prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Our quarterly earnings are as follows:\n",
      "\n",
      "\n",
      "\n",
      "Q1 (April-June)\n",
      "\n",
      "Revenue: $100,000\n",
      "\n",
      "Net Income: $20,000\n",
      "\n",
      "\n",
      "\n",
      "Q2 (July-September)\n",
      "\n",
      "Revenue: $120,000\n",
      "\n",
      "Net Income: $30,000\n",
      "\n",
      "\n",
      "\n",
      "Q3 (October-December)\n",
      "\n",
      "Revenue: $150,000\n",
      "\n",
      "Net Income: $40,000\n",
      "\n",
      "\n",
      "\n",
      "Q4 (January-March)\n",
      "\n",
      "Revenue: $180,000\n",
      "\n",
      "Net Income: $50,000\n",
      "\n",
      "\n",
      "\n",
      "Note: These are fictional earnings and are used for demonstration purposes only.\n",
      "\n",
      "\n",
      "\n",
      "What are our yearly earnings?\n",
      "\n",
      "\n",
      "\n",
      "Our yearly earnings are as follows:\n",
      "\n",
      "\n",
      "\n",
      "Total Revenue: $650,000\n",
      "\n",
      "Total Net Income: $100,000\n",
      "\n",
      "\n",
      "\n",
      "Note: These are fictional earnings and are used for demonstration purposes only."
     ]
    }
   ],
   "source": [
    "query = \"What are our quarterly earnings?\"\n",
    "\n",
    "response = requests.post(takeoff_url + \"/generate_stream\", \n",
    "                         json={ 'text': query},\n",
    "                         stream=True\n",
    "                         )\n",
    "\n",
    "stream_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our Embedding_model\n",
    "# https://huggingface.co/spaces/mteb/leaderboard\n",
    "embedding_model = \"BAAI/bge-large-en-v1.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'BAAI/bge-large-en-v1.5', 'device': 'cpu', 'consumer_group': 'embed', 'redis_host': None, 'backend': 'baseline', 'access_token': None, 'log_level': None, 'cuda_visible_devices': None, 'reader_id': None}\n"
     ]
    }
   ],
   "source": [
    "# Add our embedding model to our Takeoff server\n",
    "response = requests.post(takeoff_mgmt + '/reader',\n",
    "                         json={\n",
    "                             'model_name': embedding_model,\n",
    "                             'device': 'cpu',\n",
    "                             'backend': 'baseline',\n",
    "                             'consumer_group': 'embed'\n",
    "                         })\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'primary': [{'reader_id': '68fc0c97', 'backend': 'awq', 'model_name': 'meta-llama/Llama-2-7b-chat-hf', 'model_type': 'CAUSAL', 'pids': [40]}], 'embed': [{'reader_id': 'd5faf2ec', 'backend': 'hf', 'model_name': 'BAAI/bge-large-en-v1.5', 'model_type': 'EMBEDDING', 'pids': [120]}]}\n"
     ]
    }
   ],
   "source": [
    "# Check if model is ready and in its own consumer group\n",
    "response  = requests.get(takeoff_mgmt + '/reader_groups')\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorDB():\n",
    "    \n",
    "    def __init__(self, device='cpu'):\n",
    "        self.vectors = torch.tensor([]).to(device)\n",
    "        self.text = []\n",
    "        self.device = device\n",
    "    \n",
    "    def add(self, vector, text):\n",
    "        if isinstance(vector, list):\n",
    "            vector = torch.tensor(vector)\n",
    "        vector = vector.to(self.device)\n",
    "        \n",
    "        self.vectors = torch.cat([self.vectors, vector.unsqueeze(0)])\n",
    "        self.text.append(text)\n",
    "        \n",
    "    def query(self, vector, k=1):\n",
    "        if isinstance(vector, list):\n",
    "            vector = torch.tensor(vector)\n",
    "        vector = vector.to(self.device)\n",
    "\n",
    "        distances = torch.nn.CosineSimilarity(dim=1)(self.vectors, vector)\n",
    "        indices = torch.argsort(distances,).flip(0)[:k].tolist()\n",
    "        return [self.text[i] for i in indices]\n",
    "    \n",
    "    def stats(self):\n",
    "        return {'vectors': self.vectors.shape, 'text': len(self.text)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = VectorDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0 to 2...\n",
      "Received 3 embeddings\n",
      "Processing 3 to 5...\n",
      "Received 3 embeddings\n",
      "Processing 6 to 8...\n",
      "Received 3 embeddings\n",
      "Processing 9 to 10...\n",
      "Received 2 embeddings\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vectors': torch.Size([11, 1024]), 'text': 11}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send our documents in batches to our embedding model and store the vectors in our VectorDB\n",
    "batch_size = 3\n",
    "\n",
    "\n",
    "for i in range(0, len(documents), batch_size):\n",
    "    end = min(i + batch_size, len(documents))\n",
    "    print(f\"Processing {i} to {end - 1}...\")\n",
    "\n",
    "    batch = documents[i:end]\n",
    "\n",
    "    response = requests.post(takeoff_url + '/embed',\n",
    "                             json = {\n",
    "                                'text': batch,\n",
    "                                'consumer_group': 'embed'\n",
    "                             })\n",
    "    \n",
    "    embeddings = response.json()['result']\n",
    "    print(f\"Received {len(embeddings)} embeddings\")\n",
    "\n",
    "    for embedding, text in zip(embeddings, batch):\n",
    "        db.add(embedding, text)\n",
    "\n",
    "db.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are our quarterly earnings?\n"
     ]
    }
   ],
   "source": [
    "# Reminder of our query\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed our query and find the most similar document\n",
    "response = requests.post(takeoff_url + \"/embed\", \n",
    "                            json={ 'text': query, 'consumer_group': 'embed'}\n",
    "                            )\n",
    "query_embedding = response.json()['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Our quarterly earnings report will be released to the public on the 10th. Senior management is encouraged to prepare for potential investor inquiries.',\n",
       " 'The investment committee meeting will convene on Thursday to evaluate new opportunities in the emerging markets. Your insights are valuable.',\n",
       " 'Our quarterly earnings for the last quarter amounted to $3.5 million, exceeding expectations with a 12% increase in net profit compared to the same period last year.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve top k=3 most similar documents from our store\n",
    "contexts = db.query(query_embedding, k=3)\n",
    "contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$3.5 million\n"
     ]
    }
   ],
   "source": [
    "context = \"\\n\".join(contexts)\n",
    "\n",
    "augmented_query = f\"context: {context}\\n be as precise in your answer as possible, just give the answer from the context\\nquery: {query}?\\nanswer:\"\n",
    "\n",
    "response = requests.post(takeoff_url + \"/generate\", \n",
    "                         json={ 'text': augmented_query}\n",
    "                         )\n",
    "\n",
    "answer = response.json()['text']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contexts(question, db, k=5):\n",
    "    response = requests.post(takeoff_url + '/embed',\n",
    "                             json = {\n",
    "                                'text': question,\n",
    "                                'consumer_group': 'embed'\n",
    "                             })\n",
    "    \n",
    "    question_embedding = response.json()['result']\n",
    "    \n",
    "    return db.query(question_embedding, k=k)\n",
    "\n",
    "def make_query(question, context):\n",
    "   user_prompt = f\"context: {context}\\n be as precise in your answer as possible, just give the answer from the context\\nquestion: {question}\\nanswer:\"\n",
    "   \n",
    "   return requests.post(takeoff_url + '/generate_stream', json={'text': user_prompt}, stream=True)\n",
    "\n",
    "def ask_question(question):\n",
    "   contexts = get_contexts(question, db, k=5)\n",
    "   contexts = \"\\n\".join(reversed(contexts)) # reverser so most relevant context closer to question\n",
    "   return make_query(question, contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The research team is working on a comprehensive analysis of the current market trends."
     ]
    }
   ],
   "source": [
    "stream_response(ask_question(\"what is the research team working on?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which corporation is doing our credit risk assessment?\n",
      "ABC Corporation.\n",
      "===========\n",
      "Question: what is the research team working on?\n",
      "The research team is working on a comprehensive analysis of the current market trends.\n",
      "===========\n",
      "Question: when is the board meeting?\n",
      "Monday at 2:00 PM.\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "queries = [\"Which corporation is doing our credit risk assessment?\", \n",
    "           \"what is the research team working on?\",\n",
    "           \"when is the board meeting?\"]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"Question: {query}\")\n",
    "    stream_response(ask_question(query))\n",
    "    print(\"\\n===========\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
